---
layout: post
title: "Data Skeptics WSJ MeetUp"
date: 2014-01-15 23:17
comments: true
categories: [MeetUp, DataSkeptics]
author: Mark Veronda
---

Happy New Year, World!  I have been attending a lot of MeetUps related to data and have been learning a lot along the way.  The most recent MeetUp I attended was the [NYC Data Skeptics Society](http://www.meetup.com/The-NYC-Data-Skeptics-Meetup/events/156526602/) on January 14 hosted at Huffington Post's headquarters in NYC.  I felt it was appropriate to share with the world some of the lessons learned.

In addition to all the MeetUps attended, I recently read an excellent and inspiring book called ["Big Data: A Revolution That Will Transform How We Live, Work and Think"](http://www.amazon.com/Big-Data-Revolution-Transform-Think-ebook/dp/B009N08NKW/ref=sr_1_1?ie=UTF8&qid=1389828747&sr=8-1&keywords=big+data), written by Viktor Mayer-Sch√∂nberger and Kenneth Cukier.  Several ideas and concepts from this book came to mind while at the MeetUp.

The MeetUp featured two journalists from the Wall Street Journal, Tom McGinty and Rob Barry presenting the investigative work they did while researching [an article about the Asiana Flight 214 crash](http://online.wsj.com/article/SB10001424052702303330204579251191895420138.html).  A more appropriate title for the two might be data-driven journalists, who instead of conducting traditional journalism ala DeepThroat, actually do the work of a data scientist by finding and collecting real-world datasets and then extracting information that is then used to support their journalism. 

The most important lesson learned from this MeetUp is to be skeptical of your data throughout the full data analysis pipeline.  This ties in beautifully with the whole purpose of the Data Skeptic MeetUp group. Tom and Rob really drove home this point by demonstrating multiple times where they had data or results, but then they applied healthy skepticism only to discover it was wrong and that they had to try again.  They even went as far as to visualize over 900 examples of initial results to confirm if it was accurate.  Although this sounds extreme, they needed to be extra sure that the data supports their journalism.

There were two more key points I learned from this excellent talk.  First, there is a massive amount of extremely valuable data that is simply being thrown away.  One therefore must be resourceful to get all the data they might need to answer a question.  Secondly, two concepts from the "Big Data" book I read are highlighted by this talk.  The power of bringing in multiple datasets is exemplified by how Tom and Rob were able to make interesting conclusions beyond the initial datasets they found.   The other important concept, to quote from the book: "The era of big data challenges the way we live and interact with the world. Most strikingly, society will need to shed some of its obsession for causality in exchange for simple correlations: not knowing why but only what. This overturns centuries of established practices and challenges our most basic understanding of how to make decisions and comprehend reality." (Kindle location 108 of Chapter 1).  This was a concept that I was initially skeptical of (as most people should be), but I have now grown to accept.  Rob and Tom demonstrated this brave new world as evoked by the book because there were many points in the talk where, instead of looking for causation, they used data and correlation to guide their reporting.  Yes it's true the correlation does not imply causation, but when sifting through massive datasets and investigating new ideas, it can help tremendously with the early stages of a new investigation.

It was exciting to see data being applied to areas we don't traditionally think of as being a Data Scientist's domain.  I hope Rob and Tom represent the future of journalism, one where there is more objective, fact-based reporting that actually looks at and provides the data.  This is a much needed change I would like to see along with a shift away from instant reporting based on gut-feelings. Whenever a major event occurs, I simply refuse to watch or read anything about it beyond the intial details in real time.  The only way 'accurate' information can come out in real time reporting is through machines that can intelligently sift through and determine what is "truth."  Unfortuantely, incidents like the [Twitter Terror Error of 2013](http://www.huffingtonpost.com/mark-cuban/twitter-hackcrash_b_3202069.html) (I'm not talking about TWTR stock) suggests that we have long way to _wander_ toward that goal.
